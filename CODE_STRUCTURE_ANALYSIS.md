# Code Structure Analysis & Organization Guide

## Project Overview
This project performs **acetylcholine (ACh) imaging analysis** with two main workflows:
1. **Image Processing**: Raw image preprocessing (detrending, Gaussian filtering)
2. **Cluster Analysis**: Spike-triggered analysis with k-means clustering to identify ACh release patterns

---

## ðŸ“ Current File Structure

```
PG_005/
â”œâ”€â”€ im_process_main.py          # Main script for image preprocessing
â”œâ”€â”€ cluster_analysis.py         # Main script for spike-triggered ACh analysis
â”œâ”€â”€ classes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ plot_results.py         # GUI plotting class (PySide6 + matplotlib)
â””â”€â”€ functions/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ check_cuda.py           # CUDA availability checker
    â”œâ”€â”€ test_cuda.py            # CUDA functionality tester
    â”œâ”€â”€ get_memory_use.py       # Memory usage reporter
    â”œâ”€â”€ CPU_detrend.py          # CPU-based detrending (Numba JIT)
    â”œâ”€â”€ CPU_gauss.py            # CPU-based Gaussian blur
    â”œâ”€â”€ CPU_process.py          # CPU processing orchestrator
    â”œâ”€â”€ GPU_detrend.py          # GPU-based detrending (CUDA)
    â”œâ”€â”€ GPU_gauss.py            # GPU-based Gaussian blur (CUDA)
    â”œâ”€â”€ GPU_process.py          # GPU processing orchestrator
    â”œâ”€â”€ kmeans.py               # K-means clustering for ACh analysis
    â””â”€â”€ spatial_processing.py   # (Currently unused?)
```

---

## ðŸ”— Dependency Relationships

### **Script 1: `im_process_main.py`**
```
im_process_main.py
â”œâ”€â”€ External: imageio, numpy, numba.cuda, rich
â””â”€â”€ Internal (from functions/):
    â”œâ”€â”€ check_cuda()          â†’ Check if CUDA is available
    â”œâ”€â”€ test_cuda()           â†’ Test CUDA functionality
    â”œâ”€â”€ get_memory_usage()    â†’ Get memory consumption
    â”œâ”€â”€ process_on_gpu()      â†’ GPU processing pipeline
    â”‚   â”œâ”€â”€ gpu_detrend_jitted()
    â”‚   â””â”€â”€ gpu_gaussian_blur()
    â””â”€â”€ process_on_cpu()      â†’ CPU processing pipeline (fallback)
        â”œâ”€â”€ cpu_detrend_jitted()
        â””â”€â”€ cpu_gaussian_blur()
```

**Purpose**: Load raw TIFF stacks â†’ Detrend â†’ Gaussian blur â†’ Save results
**Output**: `*_Cal.tif` (detrended), `*_Gauss.tif` (Gaussian-filtered)

---

### **Script 2: `cluster_analysis.py`**
```
cluster_analysis.py
â”œâ”€â”€ External: imageio, numpy, pandas, matplotlib, scipy, pyabf, PySide6, rich
â”œâ”€â”€ Internal (from classes/):
â”‚   â””â”€â”€ PlotResults            â†’ Interactive Qt-based peak detection plotter
â””â”€â”€ Internal (from functions.kmeans/):
    â”œâ”€â”€ process_segment_kmeans()                    â†’ Frame-by-frame k-means
    â”œâ”€â”€ process_segment_kmeans_concatenated()       â†’ Concatenated k-means
    â””â”€â”€ visualize_clustering_results()              â†’ Multi-panel visualization
        â”œâ”€â”€ prepare_frame_for_kmeans()
        â”œâ”€â”€ apply_kmeans_to_frame()
        â”œâ”€â”€ calculate_cluster_areas()
        â””â”€â”€ split_concatenated_result()
```

**Purpose**:
1. Load ABF (electrophysiology) + TIFF (imaging) â†’ Detect spikes â†’ Segment images
2. Perform Z-score normalization using pre-spike baseline
3. Frequency-based seed pixel analysis (pixels active in â‰¥X% of segments)
4. ACh clearance analysis (spatial area changes over time)
5. K-means clustering to identify ACh release zones

**Output**: Multiple PNG figures + Excel tables

---

## ðŸ“Š Module Descriptions

### **Classes** (`classes/`)

| Module | Class | Purpose |
|--------|-------|---------|
| `plot_results.py` | `PlotResults` | Qt-based interactive window for viewing voltage traces with detected spikes (used only in cluster_analysis.py) |
| | `MplCanvas` | Helper class for matplotlib canvas in Qt |

---

### **Functions** (`functions/`)

#### **1. CUDA/GPU Management**
| File | Function | Purpose |
|------|----------|---------|
| `check_cuda.py` | `check_cuda()` | Verify CUDA availability with diagnostics |
| `test_cuda.py` | `test_cuda()` | Test GPU functionality with sample kernel |
| `get_memory_use.py` | `get_memory_usage()` | Return current process memory usage (GB) |

#### **2. Image Processing - CPU**
| File | Function | Purpose |
|------|----------|---------|
| `CPU_detrend.py` | `cpu_detrend_jitted()` | Numba JIT detrending (moving average subtraction) |
| `CPU_gauss.py` | `cpu_gaussian_blur()` | Numba JIT Gaussian blur |
| `CPU_process.py` | `process_on_cpu()` | **Orchestrator**: Warm up â†’ Detrend â†’ Gaussian blur |

#### **3. Image Processing - GPU**
| File | Function | Purpose |
|------|----------|---------|
| `GPU_detrend.py` | `gpu_detrend_jitted()` | CUDA kernel for parallel detrending |
| `GPU_gauss.py` | `gpu_gaussian_blur()` | CUDA-accelerated Gaussian blur |
| `GPU_process.py` | `process_on_gpu()` | **Orchestrator**: Transfer to GPU â†’ Detrend â†’ Gaussian blur â†’ Transfer back |

#### **4. Clustering & Analysis**
| File | Function | Purpose |
|------|----------|---------|
| `kmeans.py` | `prepare_frame_for_kmeans()` | Reshape 2D image to 1D array |
| | `apply_kmeans_to_frame()` | Run k-means on single frame, sort clusters by intensity |
| | `calculate_cluster_areas()` | Convert pixel counts to ÂµmÂ² based on magnification |
| | `visualize_clustering_results()` | Create multi-panel figure (original, clustered, spike trace) |
| | `process_segment_kmeans()` | Apply k-means frame-by-frame |
| | `process_segment_kmeans_concatenated()` | Apply k-means to horizontally concatenated frames |
| | `concatenate_frames_horizontally()` | Stack frames side-by-side |
| | `split_concatenated_result()` | Split concatenated result back to frames |

---

## ðŸ” Key Observations & Issues

### âœ… **Strengths**
1. **Clear separation**: GPU/CPU implementations are modular
2. **Graceful fallback**: CPU processing if CUDA unavailable
3. **Good documentation**: Functions have clear docstrings with "WHY" and "GOAL"
4. **Type hints**: Many functions have proper type annotations

### âš ï¸ **Issues to Address**

#### **1. Code Duplication**
- `cluster_analysis.py` has **1500 lines** with multiple analysis sections
- Repeated code patterns (creating figures, scalebars, legends)
- Hardcoded parameters scattered throughout

#### **2. Unclear Dependencies**
- `spatial_processing.py` exists but is never imported (dead code?)
- Both scripts have duplicate logging/console setup

#### **3. Mixed Responsibilities**
- `cluster_analysis.py` does:
  - Data loading
  - Spike detection
  - Segmentation
  - Z-score normalization
  - Frequency analysis
  - Clearance analysis
  - K-means clustering
  - Visualization (5+ different figure types)
  - File I/O

#### **4. Hard-to-Maintain Configuration**
```python
# Scattered throughout cluster_analysis.py:
exp_date = "2025_12_15"
magnification: str = "10X"
z_threshold = 0.25
minimal_required_frames: int = 3
maximum_allowed_frames: int = 4
TTL_5V_HIGH: float = 2.0
```

---

## ðŸ’¡ Recommended Refactoring Plan

### **Phase 1: Extract Configuration**
Create `config.py`:
```python
class AnalysisConfig:
    # Experiment parameters
    EXP_DATE = "2025_12_15"
    MAGNIFICATION = "10X"

    # Detection parameters
    TTL_HIGH_THRESHOLD = 2.0
    TTL_LOW_THRESHOLD = 0.8
    SPIKE_MIN_DISTANCE = 1500
    SPIKE_MIN_PROMINENCE = 10

    # Segmentation parameters
    MIN_REQUIRED_FRAMES = 3
    MAX_ALLOWED_FRAMES = 4

    # Analysis parameters
    Z_SCORE_THRESHOLD = 0.25
    FREQUENCY_PERCENTAGES = [50, 60, 70, 80, 90, 99, 100]
    KMEANS_CLUSTERS = 3
```

### **Phase 2: Reorganize Functions Module**
```
functions/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                 # NEW: Configuration constants
â”œâ”€â”€ hardware/                 # NEW: Group CUDA-related
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ check_cuda.py
â”‚   â”œâ”€â”€ test_cuda.py
â”‚   â””â”€â”€ get_memory_use.py
â”œâ”€â”€ preprocessing/            # NEW: Group image processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cpu_ops.py           # Merge CPU_detrend + CPU_gauss
â”‚   â”œâ”€â”€ gpu_ops.py           # Merge GPU_detrend + GPU_gauss
â”‚   â”œâ”€â”€ cpu_process.py
â”‚   â””â”€â”€ gpu_process.py
â”œâ”€â”€ spike_detection/          # NEW: Extract from cluster_analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ peak_finder.py       # Extract spike detection logic
â”‚   â””â”€â”€ segmentation.py      # Extract segmentation logic
â”œâ”€â”€ analysis/                 # NEW: Analysis functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ normalization.py     # Z-score functions
â”‚   â”œâ”€â”€ frequency_analysis.py
â”‚   â”œâ”€â”€ clearance_analysis.py
â”‚   â””â”€â”€ kmeans.py            # Keep as is
â””â”€â”€ visualization/            # NEW: Extract plotting
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ frequency_plots.py
    â”œâ”€â”€ clearance_plots.py
    â””â”€â”€ cluster_plots.py
```

### **Phase 3: Simplify Main Scripts**

**New `im_process_main.py`** (reduce to ~50 lines):
```python
from pathlib import Path
from functions.hardware import check_cuda, test_cuda
from functions.preprocessing import process_on_gpu, process_on_cpu
from functions.config import ProcessingConfig

def main():
    config = ProcessingConfig.load()

    # Check hardware
    use_gpu = check_cuda() and test_cuda()

    # Process files
    for filename in config.file_list:
        img_raw = load_image(filename)

        if use_gpu:
            detrended, gaussian = process_on_gpu(img_raw)
        else:
            detrended, gaussian = process_on_cpu(img_raw)

        save_results(filename, detrended, gaussian)

if __name__ == "__main__":
    main()
```

**New `cluster_analysis.py`** (reduce to ~200 lines):
```python
from functions.config import AnalysisConfig
from functions.spike_detection import detect_peaks, create_segments
from functions.analysis import (
    zscore_normalize,
    frequency_analysis,
    clearance_analysis,
    kmeans_clustering
)
from functions.visualization import (
    plot_frequency_results,
    plot_clearance_results,
    plot_clustering_results
)

def main():
    config = AnalysisConfig.load()

    # Load data
    abf_data, img_data = load_data(config)

    # Detect spikes
    peaks = detect_peaks(abf_data, config)

    # Create segments
    segments = create_segments(img_data, peaks, config)

    # Normalize
    normalized = zscore_normalize(segments)

    # Analyze
    freq_results = frequency_analysis(normalized, config)
    clearance_results = clearance_analysis(normalized, config)
    cluster_results = kmeans_clustering(normalized, config)

    # Visualize
    plot_frequency_results(freq_results)
    plot_clearance_results(clearance_results)
    plot_clustering_results(cluster_results)

    plt.show()

if __name__ == "__main__":
    main()
```

---

## ðŸŽ¯ Immediate Quick Wins (No Refactoring Required)

### **1. Add Module Docstrings**
Add to top of each file:
```python
"""
Module: im_process_main.py
Purpose: Preprocess raw TIFF image stacks (detrending + Gaussian filtering)
Input: Raw TIFF files from raw_images/
Output: Processed TIFF files (*_Cal.tif, *_Gauss.tif)
"""
```

### **2. Extract Magic Numbers**
At top of `cluster_analysis.py`:
```python
# Configuration (move to config.py later)
EXP_CONFIG = {
    "date": "2025_12_15",
    "magnification": "10X",
    "z_threshold": 0.25,
    # ...
}
```

### **3. Add Section Comments**
```python
# ============================================================================
# SECTION 1: DATA LOADING
# ============================================================================

# ============================================================================
# SECTION 2: SPIKE DETECTION
# ============================================================================

# etc...
```

### **4. Extract Long Functions**
Current line 753-1092 in `cluster_analysis.py` (ACh clearance analysis) â†’ extract to function:
```python
def perform_ach_clearance_analysis(
    segments: list,
    z_threshold: float,
    display_positions: list
) -> tuple[pd.DataFrame, dict]:
    """Analyze ACh clearance by measuring active area over time."""
    # Move 340 lines here
    return area_stats, avg_frames_by_position
```

---

## ðŸ“ˆ Benefits After Refactoring

| Aspect | Before | After |
|--------|--------|-------|
| **Readability** | 1500-line monolith | ~200-line orchestrator + small modules |
| **Testability** | Hard to test | Each function testable independently |
| **Reusability** | Plotting code duplicated | Reusable visualization functions |
| **Maintainability** | Change requires editing multiple places | Change config once |
| **Collaboration** | Merge conflicts likely | Clear module boundaries |

---

## ðŸš€ Implementation Priorities

### **Priority 1 (Week 1)**: Documentation & Organization
- [ ] Add module docstrings to all files
- [ ] Extract magic numbers to constants at file top
- [ ] Add clear section separators in cluster_analysis.py

### **Priority 2 (Week 2)**: Extract Configuration
- [ ] Create `config.py` with all parameters
- [ ] Update both main scripts to use config

### **Priority 3 (Week 3-4)**: Modularize Functions
- [ ] Extract spike detection â†’ `functions/spike_detection/`
- [ ] Extract visualization â†’ `functions/visualization/`
- [ ] Extract analysis â†’ `functions/analysis/`

### **Priority 4 (Week 5)**: Reorganize Function Folders
- [ ] Group CUDA functions â†’ `functions/hardware/`
- [ ] Merge CPU/GPU ops â†’ `functions/preprocessing/`

---

## ðŸ“ž Questions for You

1. **Is `spatial_processing.py` still needed?** It's not imported anywhere.
2. **Do you want to keep averaged output?** `process_on_cpu` returns 3 values but `process_on_gpu` returns 2.
3. **Should I create the refactored version?** Or just document the current structure?
4. **Testing framework**: Do you want unit tests for the refactored modules?

---

*Generated: 2026-01-12*
